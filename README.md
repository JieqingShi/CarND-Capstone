# Udacity Self Driving Car Nanodegree - Capstone Project

In the final project of the Udacity Self Driving Car Nanodegree the goal is to 
integrate the different system components to get a self-driving car running.

Due to me submitting as in individual, the code will not be tested on a real test track, only in the simulator.

The goal is to implement ROS nodes for waypoint generation, drive-by-wire and traffic light detection such that the car in the simulator is able to follow the waypoints, accelerate, decelerate, keep lanes and stop at red traffic lights. 

## Architecture
ROS nodes and topics used in this project comprise of three modules: perception, planning and control
![](imgs/final-project-ros-graph-v2.png)

### Planning
Relates to motion and trajectory planning and is responsible for updating waypoints and their associated velocities.
This module consists of the waypoint loader node (provided) and the waypoint updater node (to be implemented).

### Control
Relates to generating throttle, steering and braking signals for the car which has a drive-by-wire system.

### Perception
Relates to detecting obstacles (not needed here) and traffic lights (to be implemented)

## Project setup
In this project I have used the following setup because the provided Udacity workspace was too slow (especially when turning on the camera):

- Guest machine: The provided Ubuntu Linux VM which comes with ROS and Dataspeed DBW preinstalled, running on VirtualBox
- Host machine: Macbook Pro 2015 running the simulator on macOS Mojave 10.14.6

The VM was assigned 3 CPUs and 6GB of RAM (although the recommended 2CPUs and 4GBs is also sufficient.)


## Implementation
### Waypoint Updater Node
The following tasks are handled in the waypoint updater node:
- find nearest waypoints ahead of the vehicle. This is done using a KD-Tree algorithm to search for the nearest waypoint ahead of the vehicle. 
- for each waypoint ahead of the vehicle starting from the nearest one until the furthest one calculate the target of each waypoint. A parameter can be set to define how much we look ahead. This also includes setting the desired deceleration behavior if a red light falls within the waypoints ahead.

This node subscribes to 
``` 
rospy.Subscriber('/current_pose', PoseStamped, self.pose_cb)
rospy.Subscriber('/base_waypoints', Lane, self.waypoints_cb)
rospy.Subscriber('/traffic_waypoint', Int32, self.traffic_cb)
```

and publishes to
```
rospy.Publisher('final_waypoints', Lane, queue_size=1)
```

In my implementation I have set the lookahead horizon `LOOKAHEAD_WPS` to 50 waypoints and `rospy.Rate` to 10Hz. The reason for this setting is explained below.


### DBW Node
This node is responsible for controlling the cars steering, throttle and brakes. This can be seen as acting on the waypoints and their target velocities generated by the waypoint updater.

The node subscribes to
```
rospy.Subscriber('/vehicle/dbw_enabled', Bool, self.dbw_enabled_cb)
rospy.Subscriber('/twist_cmd', TwistStamped, self.twist_cb)
rospy.Subscriber('/current_velocity', TwistStamped, self.velocity_cb)
```

and publishes to
```
rospy.Publisher('/vehicle/steering_cmd', SteeringCmd, queue_size=1)
rospy.Publisher('/vehicle/throttle_cmd', ThrottleCmd, queue_size=1)
rospy.Publisher('/vehicle/brake_cmd', BrakeCmd, queue_size=1)
```
after applying the necessary control commands.
The publish and subscribe rate is set to 50Hz which is essential for running on the real test track and should not be changed.

### Traffic Light Classification Node
The functionality of this node is to detect traffic lights (ie. localizing bounding boxes of traffic lights and classifying their color). The detected traffic lights are published to the waypoint updater which then generates the desired target velocities based on which kind of traffic light has been detected.

These are the subscribers
```
rospy.Subscriber('/current_pose', PoseStamped, self.pose_cb)
rospy.Subscriber('/base_waypoints', Lane, self.waypoints_cb)
rospy.Subscriber('/image_color', Image, self.image_cb)
```

and the publishers of this node
```
rospy.Publisher('/traffic_waypoint', Int32, queue_size=1)
```

#### Approach
For this part I used the Tensorflow object detection API.
I also found many tutorials online regarding traffic light classification like this [one](https://github.com/alex-lechner/Traffic-Light-Classification) which contain links to pre-labeled datasets from the simulator (which would be my recommendation for getting started quickly). Having said that it's definitely also possible to collect own data by driving a few rounds in the simulator while recording the images with subsequent labeling of the images, e.g. using [labelimg](https://github.com/tzutalin/labelImg).

Regarding the model choice, I used the Single-Shot-Detector based on the Inception-Net backbone ([SSD_Inception_v2](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)) which has been trained on the COCO Dataset. 

The training process basically involved using the yaml file containing annotations to create a `.records` file and subsequently calling the `train.py` script from the API.

For inference in the simulator, the inference graph of the trained model needs to be exported with the `export_inference_graph.py` script which creates a `frozen_inference_graph.pb` file (among others). The inference graph would then need to be loaded in the corresponding python script of the ROS node.

When running the simulator, turning off "Manual" and turning on "Camera" activates autonomous driving mode with traffic light detection.

The recorded images from the simulator are then loaded into the Traffic Light detection node and processed by the trained object detection model.

The model outputs detected bounding box locations, the score of each detected object as well as their classes. After extracting the scores and comparing the highest score to the minimum detection threshold score (here 0.5 because we are only interested in confident predictions) the class corresponding to the highest score is mapped to the traffic light color and returned.

#### Performance improvement
After initial problems with the setup - which involves the export of a model trained on Tensorflow 1.14.0 using a downgraded Tensorflow version to achieve compatibility with the VM's Tensorflow 1.3.0 - I finally got the model running. 

However I would often observe a severe lag during inference, meaning that the model would only output a prediction several seconds after a traffic light has passed. This resulted in red lights being run over or prematurely stopping in front of a green light.

A possible solution is to reduce the rospy Rate in the waypoint updater node (I reduced it from 50 to 10) and also to reduce the number of lookahead waypoints (e.g. from 200 to 50) in order to minimize the computational overhead.

This improved the situation slightly (no more running over red lights) However I would still observe a lag where the car would sit still for ~3 seconds until the acceleration starts after the traffic light switches from red to green. 

What finally brought a significant performance improvement was to only load and classify every 5th image. (Implemented using a counter and when that counter reaches a multiple of 5, only then call the model inference function).
This massively reduced the reaction lag during red to green switches.

Another possibility, which I have not explored but which could be promising, is to switch to a more lightweight model such as SSD-Mobilenet. The sacrifice in prediction accuracy could be worth the speed improvement.

## Instructions

Please use **one** of the two installation options, either native **or** docker installation.

### Native Installation

* Be sure that your workstation is running Ubuntu 16.04 Xenial Xerus or Ubuntu 14.04 Trusty Tahir. [Ubuntu downloads can be found here](https://www.ubuntu.com/download/desktop).
* If using a Virtual Machine to install Ubuntu, use the following configuration as minimum:
  * 2 CPU
  * 2 GB system memory
  * 25 GB of free hard drive space

  The Udacity provided virtual machine has ROS and Dataspeed DBW already installed, so you can skip the next two steps if you are using this.

* Follow these instructions to install ROS
  * [ROS Kinetic](http://wiki.ros.org/kinetic/Installation/Ubuntu) if you have Ubuntu 16.04.
  * [ROS Indigo](http://wiki.ros.org/indigo/Installation/Ubuntu) if you have Ubuntu 14.04.
* [Dataspeed DBW](https://bitbucket.org/DataspeedInc/dbw_mkz_ros)
  * Use this option to install the SDK on a workstation that already has ROS installed: [One Line SDK Install (binary)](https://bitbucket.org/DataspeedInc/dbw_mkz_ros/src/81e63fcc335d7b64139d7482017d6a97b405e250/ROS_SETUP.md?fileviewer=file-view-default)
* Download the [Udacity Simulator](https://github.com/udacity/CarND-Capstone/releases).

### Docker Installation
[Install Docker](https://docs.docker.com/engine/installation/)

Build the docker container
```bash
docker build . -t capstone
```

Run the docker file
```bash
docker run -p 4567:4567 -v $PWD:/capstone -v /tmp/log:/root/.ros/ --rm -it capstone
```

### Port Forwarding
To set up port forwarding, please refer to the "uWebSocketIO Starter Guide" found in the classroom (see Extended Kalman Filter Project lesson).

### Usage

1. Clone the project repository
```bash
git clone https://github.com/udacity/CarND-Capstone.git
```

2. Install python dependencies
```bash
cd CarND-Capstone
pip install -r requirements.txt
```
3. Make and run styx
```bash
cd ros
catkin_make
source devel/setup.sh
roslaunch launch/styx.launch
```
4. Run the simulator

### Real world testing
1. Download [training bag](https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/traffic_light_bag_file.zip) that was recorded on the Udacity self-driving car.
2. Unzip the file
```bash
unzip traffic_light_bag_file.zip
```
3. Play the bag file
```bash
rosbag play -l traffic_light_bag_file/traffic_light_training.bag
```
4. Launch your project in site mode
```bash
cd CarND-Capstone/ros
roslaunch launch/site.launch
```
5. Confirm that traffic light detection works on real life images

### Other library/driver information
Outside of `requirements.txt`, here is information on other driver/library versions used in the simulator and Carla:

Specific to these libraries, the simulator grader and Carla use the following:

|        | Simulator | Carla  |
| :-----------: |:-------------:| :-----:|
| Nvidia driver | 384.130 | 384.130 |
| CUDA | 8.0.61 | 8.0.61 |
| cuDNN | 6.0.21 | 6.0.21 |
| TensorRT | N/A | N/A |
| OpenCV | 3.2.0-dev | 2.4.8 |
| OpenMP | N/A | N/A |

We are working on a fix to line up the OpenCV versions between the two.
